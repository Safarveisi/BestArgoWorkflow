apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: webhook
  namespace: argo-events
spec:
  template:
    serviceAccountName: operate-workflow-sa
  dependencies:
    - name: dep0
      eventSourceName: webhook
      eventName: example
  triggers:
    - template:
        conditions: "dep0"
        name: webhook-workflow-trigger
        k8s:
          operation: create
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: k8s-orchestrate-
                namespace: playground
              spec:
                serviceAccountName: playground-sa
                entrypoint: k8s-orchestrate
                artifactRepositoryRef:
                  configMap: s3-repo-config
                  key: artifactRepository
                arguments:
                  parameters:
                    - name: spark-job-name # Ensures the final step always runs if the spark job fails
                      value: spark-python
                templates:
                - name: k8s-orchestrate
                  steps:

                  - - name: random-number-job
                      template: random-number-job

                  - - name: print-generated-numbers
                      template: print-generated-numbers
                      arguments:
                        parameters:
                        - name: job-uid
                          value: '{{steps.random-number-job.outputs.parameters.job-uid}}'

                  - - name: delete-job
                      template: delete-job
                      arguments:
                        parameters:
                        - name: job-name
                          value: '{{steps.random-number-job.outputs.parameters.job-name}}'

                  - - name: generate-salary
                      template: generate-salary

                  - - name: spark-application
                      template: spark-application
                      continueOn:
                        failed: true
                      arguments:
                        parameters:
                          - name: s3-bucket
                            value: customerintelligence
                          - name: s3-script-prefix
                            value: argo/scripts/python/spark-job.py
                          - name: s3-endpoint
                            value: "http://s3-de-central.profitbricks.com"
                          - name: s3-region
                            value: "eu-central-1"
                          - name: s3-file-prefix
                            value: '{{steps.generate-salary.outputs.parameters.s3-file-prefix}}'

                  - - name: delete-spark-application
                      template: delete-spark-application

                - name: generate-salary
                  container:
                    image: alpine:latest
                    command: [sh, -c]
                    args:
                      - |
                        cat <<EOF > /tmp/salary.csv
                        id,salary
                        1,45000
                        2,90000
                        3,65000
                        4,44000
                        EOF
                  outputs:
                    artifacts:
                      - name: salary-csv
                        path: /tmp/salary.csv
                        archive:
                          none: { }
                        s3:
                          key: argo/spark/artifacts/salary.csv
                    parameters:
                      - name: s3-file-prefix
                        value: 'argo/spark/artifacts/salary.csv'

                - name: spark-application
                  inputs:
                    parameters:
                      - name: s3-bucket
                      - name: s3-script-prefix
                      - name: s3-endpoint
                      - name: s3-region
                      - name: s3-file-prefix
                  resource:
                    action: create
                    successCondition: status.applicationState.state == COMPLETED
                    failureCondition: status.applicationState.state == FAILED
                    manifest: |
                      apiVersion: sparkoperator.k8s.io/v1beta2
                      kind: SparkApplication
                      metadata:
                        name: {{workflow.parameters.spark-job-name}}
                        namespace: {{workflow.namespace}}
                      spec:
                        type: Python
                        pythonVersion: "3"
                        mode: cluster
                        image: docker.io/ciaa/spark-jobs:v0.1.0
                        imagePullPolicy: Always
                        mainApplicationFile: s3a://{{inputs.parameters.s3-bucket}}/{{inputs.parameters.s3-script-prefix}}
                        arguments:
                          - "--s3key"
                          - "{{inputs.parameters.s3-bucket}}/{{inputs.parameters.s3-file-prefix}}"
                        sparkConf:
                          spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
                          spark.hadoop.fs.s3a.path.style.access: "true"
                          spark.hadoop.fs.s3a.endpoint: {{inputs.parameters.s3-endpoint}}
                          spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
                          spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
                          spark.hadoop.fs.s3a.endpoint.region: {{inputs.parameters.s3-region}}
                        sparkVersion: 4.0.1
                        restartPolicy:
                          type: Never
                        driver:
                          configMaps:
                            - name: spark-dummy
                              path: /mnt/dummy
                          cores: 1
                          memory: 512m
                          serviceAccount: spark-operator-spark
                          securityContext:
                            capabilities:
                              drop: [ALL]
                            runAsGroup: 185
                            runAsUser: 185
                            runAsNonRoot: true
                            allowPrivilegeEscalation: false
                            seccompProfile:
                              type: RuntimeDefault
                          env:
                            - name: AWS_ACCESS_KEY_ID
                              valueFrom:
                                secretKeyRef:
                                  name: s3-credentials
                                  key: accessKey
                            - name: AWS_SECRET_ACCESS_KEY
                              valueFrom:
                                secretKeyRef:
                                  name: s3-credentials
                                  key: secretKey
                        executor:
                          instances: 1
                          cores: 1
                          memory: 512m
                          securityContext:
                            capabilities:
                              drop: [ALL]
                            runAsGroup: 185
                            runAsUser: 185
                            runAsNonRoot: true
                            allowPrivilegeEscalation: false
                            seccompProfile:
                              type: RuntimeDefault
                          env:
                            - name: AWS_ACCESS_KEY_ID
                              valueFrom:
                                secretKeyRef:
                                  name: s3-credentials
                                  key: accessKey
                            - name: AWS_SECRET_ACCESS_KEY
                              valueFrom:
                                secretKeyRef:
                                  name: s3-credentials
                                  key: secretKey

                - name: random-number-job
                  resource:
                    action: create
                    successCondition: status.succeeded > 9
                    failureCondition: status.failed > 0
                    manifest: |
                      apiVersion: batch/v1
                      kind: Job
                      metadata:
                        generateName: rand-num-
                        namespace: {{workflow.namespace}}
                      spec:
                        completions: 10
                        parallelism: 10
                        template:
                          metadata:
                            name: rand
                          spec:
                            containers:
                            - name: rand
                              image: python:alpine3.6
                              command: ["python", "-c", "import random; import time; print(random.randint(1, 1000)); time.sleep(10)"]
                            restartPolicy: Never
                  outputs:
                    parameters:
                    - name: job-name
                      valueFrom:
                        jsonPath: '{.metadata.name}'
                    - name: job-uid
                      valueFrom:
                        jsonPath: '{.metadata.uid}'

                - name: print-generated-numbers
                  inputs:
                    parameters:
                    - name: job-uid
                  container:
                    image: bitnami/kubectl:latest
                    command: [sh, -c]
                    args:
                      - |
                        for pod in $(kubectl get pods -n {{workflow.namespace}} \
                                  -l controller-uid={{inputs.parameters.job-uid}} \
                                  -o name); do
                          kubectl logs -n {{workflow.namespace}} "$pod"
                        done

                - name: delete-job
                  inputs:
                    parameters:
                    - name: job-name
                  resource:
                    action: delete
                    manifest: |
                      apiVersion: batch/v1
                      kind: Job
                      metadata:
                        name: {{inputs.parameters.job-name}}
                        namespace: {{workflow.namespace}}

                - name: delete-spark-application
                  resource:
                    action: delete
                    manifest: |
                      apiVersion: sparkoperator.k8s.io/v1beta2
                      kind: SparkApplication
                      metadata:
                        name: {{workflow.parameters.spark-job-name}}
                        namespace: {{workflow.namespace}}
